{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 입력 데이터 파이프라인 구축\n",
    "## 1) tf.data.Dataset 데이터\n",
    "### Motivation\n",
    "기존의 데이터는 `host memory` 위에 있지만 `tf.data.Dataset` 객체 속 데이터는 GPU에 존재함\n",
    "### 데이터 로드\n",
    "`tensorflow_datasets`을 이용해 데이터 로드\n",
    "\n",
    "- 사실 디스크에 있는 텍스트 테이터를 Dataset으로 로드하기 위해선 `tf.keras.utils.text_dataset_from_directory`가 필요하지만, 저는 `tensorflow_datasets` 을 통해 Datasets 객체를 바로 생성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 구조\n",
    "- `Dataset.element_spec`을 통해 확인가능\n",
    "- `Dataset.element_spec`의 결과의 시퀀스로 구성됨\n",
    "> 예를 들어 `TensorSpec(shape=(10,), dtype=tf.float32, name=None)` 라면 이 데이터는 10개의 차원을 가진 벡터들의 이터러블한 조합이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "text:  tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "label:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 15:03:55.532798: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 데이터 셋 요소 배치 처리\n",
    "- 입력 데이터를(여러 문장을) 배치 단위로 나누어 학습하기 위해\n",
    "- `BUFFER_SIZE` 안에서 랜덤으로 `BATCH_SIZE` 만큼을 선택\n",
    "- `.reshape()`과 비슷하다.\n",
    "- 예를 들어(`BATCH_SIZE` = 2), 1 * 10 -> 5 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "# train_dataset만 shuffle하는 이유?\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train_dataset`: [([text1, text2, ...], [label1, label2, ...]), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'How come I\\'ve never seen or even heard about this junk-movie before? It\\'s right up my alley with bloody teenkill, laughable plotting and an irresistible 80\\'s cheese-atmosphere hanging around it. For some reason nobody is really interested in, the staff and students of an elite Catholic university are butchered by an unknown psychopath. Freshly recruited teacher Julie Parker becomes involved when all the people she has contact with either turn up dead or behave strangely. This movie is hilariously bad! There\\'s absolutely no logic or coherence and every character is equally meaningless to the others. For example, there\\'s a girl killed and her body dumped in a container. Then, and for no reason, the story suddenly moves forward three weeks yet the murdered girl is never mentioned or even missed. Not even by her boyfriend! The acting is pitiful and there isn\\'t even a bit of nudity to enjoy. The revelation of the killer is quite funny because the makers really seemed convinced that it was an original twist... It\\'s not, guys! \"Splatter University\" is easily one of the worst horror-turkeys ever.'\n",
      " b'This film is a flagrant rip-off of one of the best novels of all time, Silas Marner by George Eliot.<br /><br />The details of the film shown on IMDb do give acknowledgement to the original authoress but I did not see this at the beginning of the film, only a credit at the end of it saying \"suggested by the book Silas Marner\". Suggested? It was nothing but a complete rip- off of all the essential elements of the story:<br /><br />A wronged and sad old man, an artisan, poor and lonely, has all his money stolen. One night a child wanders up to his door as her mother lies dying in the snow outside. The man takes her in and brings her up until one day the local squire (or rich politician here) demands to adopt the child. It is he who has fathered the child during an illicit affair years before. The battle then ensues as to who should have legal custody of the child.<br /><br />In this and every other aspect of the film, the story is exactly the same. In only one can I find a difference. Silas Marner had epilepsy - but perhaps that would have strained the acting abilities of Mr Martin too far. On top of that he has his hair dyed in some carrot juice concoction (presumably to make him look younger, but actually making him look more the clown that he is)! There is also the addition of meaningless jokes, that this offbeat comedian cannot resist bringing into the story which have no part in it and only detract from the profoundness of the story. Like when the child cries in the courthouse declaring she can only be happy with the man who has fathered her all these years. This is conveyed in the film by the girl applying nasal decongestant to the bridge of her nose to make her tearful!<br /><br />I am surprised that legalities and integrity within the film industry permit such a film to be made. If I was a trustee of George Eliot\\'s I would insist on reparation. If I was Steve Martin I would send the profits to that estate, or to the poor. At the very least it should be entitled Silas Marner - adapted by S Martin. Or better still removed from the archives!<br /><br />If you are interested in this story - and I hope you are - dismiss this completely and watch Silas Marner. Or read the book! The BBC made an excellent adaptation of it in the 1980\\'s.']\n",
      "\n",
      "labels:  [0 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 21:40:06.591767: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:2])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) TextVectorization 레이어 \n",
    "- 입력 데이터 (한 문장) -> `one-hot encording`벡터의 집합 -> 토큰 인덱스의 시퀀스 (단어 집합 이용)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 21:40:10.658789: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 집합 구하기 (모든 입력 데이터 -> corpus 빈도 수 -> 단어 집합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`64`개의 문장은 `988` 길이의 `Int[]` 로 표현됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 988)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example).numpy()\n",
    "encoded_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Embedding\n",
    "- 단어 인덱스 시퀀스를 벡터 시퀀스로 변환\n",
    "- 인풋(원 핫 벡터)의 길이와 아웃풋(임베딩 벡터 길이) 길이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 구성\n",
    "## 1) sequential\n",
    "하나의 입력(한 문장)에 대해 하나의 출력(긍정 or 부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 모델 생성 \n",
    "### RNN\n",
    "- 입력 단어 벡터: `xt`\n",
    "- hidden state vector: `ht`\n",
    "- `ht` = `tanh(ht-1 x Wh + xt x Wx + b)`\n",
    "- 다대일 구조 (`return_sequences=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64 # hidden state vector 크기\n",
    "RNN_layer = tf.keras.layers.SimpleRNN(hidden_units )\n",
    "# RNN_layer = tf.keras.layers.SimpleRNN(\n",
    "#     hidden_units,\n",
    "#     activation='tanh',\n",
    "#     use_bias=True,\n",
    "#     kernel_initializer='glorot_uniform',\n",
    "#     recurrent_initializer='orthogonal',\n",
    "#     bias_initializer='zeros',\n",
    "#     kernel_regularizer=None,\n",
    "#     recurrent_regularizer=None,\n",
    "#     bias_regularizer=None,\n",
    "#     activity_regularizer=None,\n",
    "#     kernel_constraint=None,\n",
    "#     recurrent_constraint=None,\n",
    "#     bias_constraint=None,\n",
    "#     dropout=0.0,\n",
    "#     recurrent_dropout=0.0,\n",
    "#     return_sequences=False,\n",
    "#     return_state=False,\n",
    "#     go_backwards=False,\n",
    "#     stateful=False,\n",
    "#     unroll=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 64)                8256      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,256\n",
      "Trainable params: 72,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(RNN_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary classification\n",
    "- sigmoid 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_3 (TextV  (None, None)             0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,321\n",
      "Trainable params: 72,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/JM_ML/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n",
      "2022-08-11 08:52:52.431815: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('JM_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587c7040e04ce22e7cb78113e365c957ceaea79c8bf9f36315c504e66beb96ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
