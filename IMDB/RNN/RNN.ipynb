{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 입력 데이터 파이프라인 구축\n",
    "## 1) tf.data.Dataset 데이터\n",
    "### Motivation\n",
    "기존의 데이터는 `host memory` 위에 있지만 `tf.data.Dataset` 객체 속 데이터는 GPU에 존재함\n",
    "### 데이터 로드\n",
    "`tensorflow_datasets`을 이용해 데이터 로드\n",
    "\n",
    "- 사실 디스크에 있는 텍스트 테이터를 Dataset으로 로드하기 위해선 `tf.keras.utils.text_dataset_from_directory`가 필요하지만, 저는 `tensorflow_datasets` 을 통해 Datasets 객체를 바로 생성했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 09:02:48.165340: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-11 09:02:48.165675: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
    "                          as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 구조\n",
    "- `Dataset.element_spec`을 통해 확인가능\n",
    "- `Dataset.element_spec`의 결과의 시퀀스로 구성됨\n",
    "> 예를 들어 `TensorSpec(shape=(10,), dtype=tf.float32, name=None)` 라면 이 데이터는 10개의 차원을 가진 벡터들의 이터러블한 조합이라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 09:02:48.418952: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-11 09:02:48.437313: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('text: ', example.numpy())\n",
    "  print('label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 데이터 셋 요소 배치 처리\n",
    "- 입력 데이터를(여러 문장을) 배치 단위로 나누어 학습하기 위해\n",
    "- `BUFFER_SIZE` 안에서 랜덤으로 `BATCH_SIZE` 만큼을 선택\n",
    "- `.reshape()`과 비슷하다.\n",
    "- 예를 들어(`BATCH_SIZE` = 2), 1 * 10 -> 5 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "# train_dataset만 shuffle하는 이유?\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train_dataset`: [([text1, text2, ...], [label1, label2, ...]), ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:  [b'My older sister was born in March of 1985 and has cerebral palsy. in her 22 years of life, she has seen nothing but the walls of our house and her school which is also occupied with other disabled kids. i have been the butt of everyone\\'s jokes because my sister is disabled, and i still think to this day that nobody is, or ever will give a damn about her and her condition. Then i saw this film.<br /><br />I knew what Christy\\'s family was going through. but they were lucky. Christy could talk, he could communicate, and he had artistic skills. my sister can walk, but she can\\'t utter a word, and she can\\'t use her hands to do anything but grab onto things. but this film made me realize there were other people in the world like my sister, and the ending (to tell the truth) made me cry. AND I\\'VE SEEN SHAWSHANK!!! This film is seriously underrated, and it shouldn\\'t. This movie tells people something. that people should be proud of their own lives. thinking you can\\'t write well? this guy wrote with his foot. thinking you\\'re not attractive? this guy got turned down by lots of girls, because of his condition. not the fastest runner? christy couldn\\'t even stand up.<br /><br />My point: Parents of young children, i suggest your children watch this movie with you, so they\\'ll know the next time they see someone on the street in a wheelchair, they don\\'t stare at them like they\\'re aliens. My sister got millions of stares, and it breaks my heart to think that this is still happening to many people. This film will teach people, that people who might not seem \"normal\" are people too. 10/10'\n",
      " b\"I liked this show! I think it was nothing with wrong with it! Only that Spidey don't punch anyone but only for that the show doesn't suck! Some people only think this show is bad because of that. The story was great and it was fun when other heroes appeared like X-men, The Punisher, Daredevil and Iron Man! To bad Sandman never appear but i kinda like it! Best Spidey show ever!! My favorite episodes are: 1. Turning Point 2. Spider Wars 3. The Hobgoblin 4. The Alien Costume 5. Mutant Agenda<br /><br />But there are some episodes that was really really bad like: Rocket Racer and The Spot which was embarrassing to watch. And i don't like Morbius and Hydro Man. First of Morbius suck plasma instead of blood and i don't like vampires. And it irritates me that he was almost the main villain in Season 2. Of course i have to mentioned Hydro Man! He was terrible! I rather see Sandman! His last appearance was so terrible. And i don't like Spidey as the Man-spider!<br /><br />But i guess everything than this was bad!\"]\n",
      "\n",
      "labels:  [1 0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 09:02:48.749531: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for example, label in train_dataset.take(1):\n",
    "  print('texts: ', example.numpy()[:2])\n",
    "  print()\n",
    "  print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) TextVectorization 레이어 \n",
    "- 입력 데이터 (한 문장) -> `one-hot encording`벡터의 집합 -> 토큰 인덱스의 시퀀스 (단어 집합 이용)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-11 09:02:48.875576: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 집합 구하기 (모든 입력 데이터 -> corpus 빈도 수 -> 단어 집합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`64`개의 문장은 `988` 길이의 `Int[]` 로 표현됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1002)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_example = encoder(example).numpy()\n",
    "encoded_example.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Embedding\n",
    "- 단어 인덱스 시퀀스를 벡터 시퀀스로 변환\n",
    "- 인풋(원 핫 벡터)의 길이와 아웃풋(임베딩 벡터 길이) 길이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 모델 구성\n",
    "## 1) sequential\n",
    "하나의 입력(한 문장)에 대해 하나의 출력(긍정 or 부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) 모델 생성 \n",
    "### RNN\n",
    "- 입력 단어 벡터: `xt`\n",
    "- hidden state vector: `ht`\n",
    "- `ht` = `tanh(ht-1 x Wh + xt x Wx + b)`\n",
    "- 다대일 구조 (`return_sequences=False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_units = 64 # hidden state vector 크기\n",
    "RNN_layer = tf.keras.layers.SimpleRNN(hidden_units)\n",
    "# RNN_layer = tf.keras.layers.SimpleRNN(\n",
    "#     hidden_units,\n",
    "#     activation='tanh',\n",
    "#     use_bias=True,\n",
    "#     kernel_initializer='glorot_uniform',\n",
    "#     recurrent_initializer='orthogonal',\n",
    "#     bias_initializer='zeros',\n",
    "#     kernel_regularizer=None,\n",
    "#     recurrent_regularizer=None,\n",
    "#     bias_regularizer=None,\n",
    "#     activity_regularizer=None,\n",
    "#     kernel_constraint=None,\n",
    "#     recurrent_constraint=None,\n",
    "#     bias_constraint=None,\n",
    "#     dropout=0.0,\n",
    "#     recurrent_dropout=0.0,\n",
    "#     return_sequences=False,\n",
    "#     return_state=False,\n",
    "#     go_backwards=False,\n",
    "#     stateful=False,\n",
    "#     unroll=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                8256      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,256\n",
      "Trainable params: 72,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(RNN_layer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### binary classification\n",
    "- sigmoid 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, None)             0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 64)          64000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 64)                8256      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72,321\n",
      "Trainable params: 72,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/JM_ML/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  return dispatch_target(*args, **kwargs)\n",
      "2022-08-11 09:02:51.196675: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('JM_ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587c7040e04ce22e7cb78113e365c957ceaea79c8bf9f36315c504e66beb96ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
